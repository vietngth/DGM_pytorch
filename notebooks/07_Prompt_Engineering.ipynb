{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SbCECJirG3m"
      },
      "source": [
        "<div class='bar_title'></div>\n",
        "\n",
        "*Practical Data Science*\n",
        "\n",
        "# Prompt Engineering\n",
        "\n",
        "Gunther Gust & Viet Nguyen<br>\n",
        "Chair of Enterprise AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stX89-8qvami"
      },
      "source": [
        "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/d3.png?raw=true\" style=\"width:20%; float:left;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR7fJorDyiYh"
      },
      "source": [
        "<img src=\"https://github.com/GuntherGust/practical_data_science/blob/main/notebooks/images/CAIDASlogo.png?raw=1\" style=\"width:20%; float:left;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k1KMDc-rG3n"
      },
      "source": [
        "In this lecture, we will introduce the concept of prompt engineering used in large language models (LMs). All examples are demonstrated using [Gemini APIs](https://ai.google.dev/gemini-api/docs), and the lecture mainly follows the teaching materials of [DAIR.AI](https://github.com/dair-ai/Prompt-Engineering-Guide/tree/main)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5cjR5F4rG3n"
      },
      "source": [
        "## Table of Contents\n",
        "1. Basics of prompt engineering\n",
        "2. Several advanced techniques for more complex prompts\n",
        "3. General tips for designing prompts\n",
        "4. Tools for playing around with prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QEWeRiGvamj"
      },
      "source": [
        "## 1. Basics of Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_BqCnlhhai-",
        "outputId": "bf39f7b9-b1c4-4f25-e01d-24004e4ba208"
      },
      "source": [
        "Prompt engineering is a field of creating and optimizing prompts to effectively utilize and enhance large language models (LLMs) across many applications. Developing skills in prompt engineering allows practitioners to gain deeper insights into the strengths and limitations of LLMs. Researchers leverage these techniques to enhance the safetey and performance of LLMs, aiming to deal with a varetiery of tasks, from straightforward question answering to complex arithmetic reasoning. Meanwhile, developers employ prompt engineering to craft robust and effective prompting strategies to interact with LLMs and other tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using `Gemini APIs`, you need to create a secret key [here](https://aistudio.google.com/app/apikey). Please keep the secret key somewhere safe because you cannot retrieve it on the website again. Although this is not a good practice, let's try to use the secret key directly here for simplicity:"
      ],
      "metadata": {
        "id": "YtryBVPL1Hhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# INSERT YOUR KEY HERE\n",
        "key = \"AIzaSyAGWyzerTRj94wHnfz7BvyLVkxCbusLzfo\"\n",
        "\n",
        "# configure the key for calling GenAI model\n",
        "genai.configure(api_key=key)\n",
        "\n",
        "# load model\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "NsqO5Y481coL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to new policy of limiting data usage from `Open APIs`, we utilize the examples of [DAIR.AI](https://github.com/dair-ai/Prompt-Engineering-Guide/tree/main) with Google Model `Gemini` instead. You can take a look at all variants [here](https://ai.google.dev/gemini-api/docs/models/gemini) (You need a Google Account). In this lecture, we use the standard `Gemini 1.5 Flash` that has great performance for most tasks, including images, video, and text."
      ],
      "metadata": {
        "id": "qnO90ggJ_T3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Basic Prompting\n",
        "With simple prompts, you can achieve reasonable results, but the outcome largely depends on how well you structure your request and the amount of detail you include. A well-thought-out prompt goes beyond just a basic question or instruction; it incorporates essential information like context, examples, or specifics. You can make the model understand your request better and enhance the quality of the response.\n",
        "\n",
        "Below is a simple prompt example:"
      ],
      "metadata": {
        "id": "-yxGpO924j4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "prompt = \"The sky is\"\n",
        "\n",
        "#response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "8E5YjF7H4iVR",
        "outputId": "3e836cb5-86a5-4c42-a8f5-916824915df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky is blue.  (But it can also be many other colors depending on the time of day and weather conditions.)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Text Summarization"
      ],
      "metadata": {
        "id": "dXAXcyGP-GpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections.\n",
        "They work by either killing the bacteria or preventing them from reproducing,\n",
        "allowing the body's immune system to fight off the infection.\n",
        "Antibiotics are usually taken orally in the form of pills, capsules,\n",
        "or liquid solutions, or sometimes administered intravenously.\n",
        "They are not effective against viral infections,\n",
        "and using them inappropriately can lead to antibiotic resistance.\n",
        "\n",
        "Explain the above in one sentence:\"\"\"\n",
        "\n",
        "# response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "OEztdSEy4iYX",
        "outputId": "ad4c8f5b-8e48-411e-af0a-d876ec09495e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antibiotics are medications that kill or stop the reproduction of bacteria, helping the body fight bacterial infections, but are ineffective against viruses and their overuse can cause resistance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Question and Answering\n"
      ],
      "metadata": {
        "id": "DvtZEYKv-YQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "prompt = \"\"\"Answer the question based on the context below.\n",
        "Keep the answer short and concise.\n",
        "Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company\n",
        "called Ortho Pharmaceutical. There, scientists generated an early version\n",
        "of the antibody, dubbed OKT3. Originally sourced from mice, the molecule\n",
        "was able to bind to the surface of T cells and limit their cell-killing\n",
        "potential. In 1986, it was approved to help prevent organ rejection\n",
        "after kidney transplants, making it the first therapeutic antibody\n",
        "allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\n",
        "Explain the above in one sentence:\"\"\"\n",
        "\n",
        "#response\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "PwwzvSDr-dT0",
        "outputId": "319889ba-ee02-4c32-cd3a-ddf475e8dab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mice.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context is taken from [here](https://www.nature.com/articles/d41586-023-00400-x)."
      ],
      "metadata": {
        "id": "VZ6GCGOS-0jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Text Classification"
      ],
      "metadata": {
        "id": "HWNZeJ1z-749"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "Text: I think the food was okay.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "# response\n",
        "response = model.generate_content(prompt)\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "ctvRi2_5-2qH",
        "outputId": "d28e95dc-4824-437b-fccb-af02479ef973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sentiment: **Neutral**\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Short exercise: Tweaking the prompt\n",
        "Modify the above text to make the sentiment into \"Negative\". Note that sometimes the model outputs normal text without markdown format, and it is fine. You can enforce your prompt to format the text."
      ],
      "metadata": {
        "id": "TUZgqoLfAM2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your prompt here\n",
        "prompt1 = \"\"\"Classify the text into neutral, negative or positive.\n",
        "Text: I think the food was bad.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"Classify the text into neutral, negative or positive. Make the answer bold.\n",
        "Text: I think the food was bad.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "# response\n",
        "response = model.generate_content(prompt1)\n",
        "display(Markdown(response.text))\n",
        "\n",
        "response = model.generate_content(prompt2)\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "u2EN8bT2AS3U",
        "outputId": "a17a2597-5009-41c4-d892-3126c71bb7ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sentiment: Negative\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sentiment: **Negative**\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Role Playing"
      ],
      "metadata": {
        "id": "SJFs1hbk_NkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "prompt = \"\"\"The following is a conversation with an AI research assistant.\n",
        "The assistant tone is technical and scientific.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\"\"\"\n",
        "\n",
        "# response\n",
        "response = model.generate_content(prompt)\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "wOAzXK_zAfc1",
        "outputId": "d4a3b86b-2914-445d-86ad-9a6e347d3eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The creation of black holes is a complex process governed by the principles of general relativity.  While various mechanisms exist, they all involve the gravitational collapse of a sufficiently massive object.  The most widely accepted pathways are:\n\n* **Stellar Black Hole Formation:** This is the most common pathway.  Massive stars (generally those exceeding 20-30 solar masses, though the exact limit is dependent on stellar metallicity and rotation)  exhaust their nuclear fuel.  The outward pressure from nuclear fusion no longer counteracts the inward pull of gravity.  The core collapses catastrophically, leading to a supernova explosion. If the remnant core's mass exceeds the Tolman–Oppenheimer–Volkoff limit (approximately 2-3 solar masses),  further collapse ensues, resulting in a black hole.  The precise details depend on the star's initial mass, composition, and rotation.\n\n* **Supermassive Black Hole Formation:** The formation of supermassive black holes (SMBHs), millions or even billions of solar masses, remains an area of active research.  Several theories exist, including:\n    * **Direct Collapse:** In the early universe, under specific conditions of high density and low metallicity, gas clouds could have collapsed directly into SMBHs without forming intermediate-mass stars.\n    * **Seed Black Holes and Accretion:**  Stellar-mass black holes or intermediate-mass black holes formed through stellar collapse could act as \"seeds,\" gradually accumulating matter through accretion to become supermassive over cosmic timescales.  This process is likely facilitated by mergers of galaxies and their central black holes.\n    * **Hierarchical Merging:** Multiple smaller black holes merging together over time, a process potentially accelerated by galaxy mergers.\n\n* **Primordial Black Holes:** These hypothetical black holes formed in the very early universe, possibly from density fluctuations in the aftermath of the Big Bang. Their existence remains unproven, although some cosmological models predict their formation.\n\n\nIt's crucial to note that these formation processes are not mutually exclusive.  A given SMBH might have formed through a combination of direct collapse and subsequent accretion of matter.  Furthermore, our understanding of black hole formation is continuously being refined with new observations and theoretical advancements.  The specifics of each formation pathway involve complex astrophysical processes, including hydrodynamic simulations, magnetohydrodynamic effects, and general relativistic calculations that are beyond the scope of a brief summary.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Code Generation (SQL)"
      ],
      "metadata": {
        "id": "Mho6_KNSBx0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a MySQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"\n",
        "\n",
        "# response\n",
        "response = model.generate_content(prompt)\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "LBFWCd9DBxUH",
        "outputId": "8553860f-8fb8-4190-a28c-c4af4020ec6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```sql\nSELECT\n    StudentId,\n    StudentName\nFROM\n    students\nWHERE\n    DepartmentId = (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science');\n```\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 Reasoning"
      ],
      "metadata": {
        "id": "wRwj-UH2CBds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve by breaking the problem into steps.\n",
        "First, identify the odd numbers, add them, and indicate\n",
        "whether the result is odd or even.\"\"\"\n",
        "\n",
        "# response\n",
        "response = model.generate_content(prompt)\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "nNATVG-TCAVD",
        "outputId": "20c5a389-8f78-4677-862b-1b7e0cee694e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve the problem step-by-step:\n\n**Step 1: Identify the odd numbers.**\n\nThe odd numbers in the group are: 15, 5, 13, 7, 1\n\n**Step 2: Add the odd numbers.**\n\n15 + 5 + 13 + 7 + 1 = 41\n\n**Step 3: Indicate whether the result is odd or even.**\n\nThe result, 41, is an **odd** number.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8zjU2YDpEX8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Short Exercise: Caption an image"
      ],
      "metadata": {
        "id": "R_TVhn-GC0YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beside text-to-text format, you can also generate text for a given input image using `Gemini 1.5 Pro`.\n",
        "\n",
        "Your task: create a prompt to make a caption for the image. The response MUST contain a list of bullet points."
      ],
      "metadata": {
        "id": "P3UZhZ9aD8mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "import os\n",
        "import base64\n",
        "\n",
        "model = genai.GenerativeModel(model_name = \"gemini-1.5-pro\")\n",
        "image_path = \"https://miro.medium.com/v2/resize:fit:720/format:webp/1*tjPh2MVUFSdqREruQCuurQ.jpeg\"\n",
        "\n",
        "image = httpx.get(image_path)\n",
        "\n",
        "# Give it a prompt -- YOUR CODE HERE\n",
        "prompt = \"Caption this image, describe the characteristics with a list of bullet points.\"\n",
        "response = model.generate_content([{'mime_type':'image/jpeg', 'data': base64.b64encode(image.content).decode('utf-8')}, prompt])\n",
        "\n",
        "# Print the caption in the markdown format -- YOUR CODE HERE\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "MPlu8bSaDN5r",
        "outputId": "ad8077f4-8daf-4b34-bb68-d0023e4f9b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This image appears to be fan art of a cartoon dog, likely inspired by the animation style of Genndy Tartakovsky (creator of Dexter's Laboratory, Samurai Jack, and Primal). It has an air of nonchalant coolness or swagger.\n\n* **Animal:** A stylized dog or canine-like creature.\n* **Expression:** The character has a sly, almost smug expression, emphasized by the half-closed eyes.\n* **Attire:** It wears a simple, oversized gray sweater, light blue jeans, and red sneakers.  The jeans are rolled up slightly at the ankles.\n* **Pose:** The character stands with its hands in its pockets, further contributing to the laid-back attitude.\n* **Color Palette:** The image uses a limited color palette, mostly browns, blues, grays, and reds, against a muted green background.\n* **Style:**  The drawing style is reminiscent of early 2000s cartoons, with thick lines and somewhat simplified shapes. The shading and coloring appear slightly rough, giving it a sketched or hand-drawn feel."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MfGBqqfLGRIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. More Advanced Prompting Techniques\n"
      ],
      "metadata": {
        "id": "danzkj_KCXcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Zero-shot Prompting\n",
        "\n",
        "Large language models such as GPT-3.5 Turbo, GPt-4, Claude 3 and Gemini are trained on large and diverse datasets. This large-scale training setting enables these models to handle certain tasks using a \"zero-shot\" approach. In zero-shot prompting, the input provided to the model contains no examples or demonstrations. Instead, the prompt gives direct instructions for the task, relying solely on the model's inherent capabilities to understand and execute it. All of the examples you see above are `zero-shot` prompting. Here is another zero-shot `text classification` example:\n"
      ],
      "metadata": {
        "id": "JOhC12O6CwF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "Text: I enjoyed the concert last night, although the technical issues took an hour to be resolved.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "# response\n",
        "response = model.generate_content(prompt)\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "fKLNeIzJGdqs",
        "outputId": "3ffba523-de05-450d-fab5-9e30a59deb09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sentiment: **Positive**\n\nDespite the negative experience of the technical issues, the overall sentiment is positive because the user explicitly states they \"enjoyed the concert\". The negative aspect is presented as a mitigating factor, not the overriding experience.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Few-shot Prompting"
      ],
      "metadata": {
        "id": "Jw2lqBR2E4IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Chain-of-Thought Prompting"
      ],
      "metadata": {
        "id": "Z3ZQb3alE4UN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjZT1mMPFbyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Meta Prompting"
      ],
      "metadata": {
        "id": "k9yo77QvFcMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Generate Knowledge Prompting"
      ],
      "metadata": {
        "id": "kWG0dk8eFcZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Automatic Prompt Engineer (Bonus)"
      ],
      "metadata": {
        "id": "_uOKtxuYFjyl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkJhMj3Tvamv"
      },
      "source": [
        "Outro"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7rH6rLdeCsWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn_BiQqPyiYi"
      },
      "source": [
        "\n",
        "<img src=\"https://github.com/GuntherGust/practical_data_science/blob/main/notebooks/images/d3.png?raw=1\" style=\"width:50%; float:center;\" />\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}